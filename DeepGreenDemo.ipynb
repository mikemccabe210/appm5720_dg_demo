{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28dbb055",
   "metadata": {
    "id": "28dbb055"
   },
   "source": [
    "# APPM 5720 DeepGreen Example Notebook\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mikemccabe210/appm5720_dg_demo/blob/main/DeepGreenDemo.ipynb)\n",
    "#### Paper: DeepGreen: deep learning of Greenâ€™s functions for nonlinear boundary value problems\n",
    "#### Paper by: Craig R. Gin, Daniel E. Shea, Steven L. Brunton, and J. Nathan Kutz\n",
    "#### Notebook by: Rey Koki and Mike McCabe\n",
    "\n",
    "This notebook walks through the nonlinear Poisson problem from _DeepGreen_. Several steps are slightly simplified from the original code, which can be found here: https://github.com/sheadan/DeepGreen, since we're only implementing one example and do not need the fully general code. We also make a few small departures for efficiency/speed purposes. These will be pointed out when they come up.\n",
    "\n",
    "## Nonlinear Poisson\n",
    "\n",
    "We experiment with the paper's choice of a nonlinear version of the Poissson equation with Dirichlet boundary conditions. \n",
    "\n",
    "$-\\nabla \\cdot [(1+u^2)\\nabla u] = F(x), \\: \\: \\: \\: \\: \\: \\: x\\in\\Omega$\n",
    "\n",
    "where \n",
    "\n",
    "$ u = 0, \\: \\: \\: \\: \\: \\: x\\in \\partial \\Omega$\n",
    "\n",
    "Using the method of manufactured solutions, we build a library of cosine and Gaussian functions. These functions are then used as the true solutions and forcing functions that are fed into the neural network during the training process (see \"Building the data set\" section).\n",
    "\n",
    "For analyzing the trained model, we look at testing data created using the same family (cosine and Gaussian) of functions. Additionally, we test on cubic functions that were never seen during training. We added capabilities that allow for the user to test out their own function in order to experiment with the model's ability to generalize to new functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fb60d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optax flax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083b497f",
   "metadata": {
    "id": "083b497f"
   },
   "outputs": [],
   "source": [
    "# Imports!\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.random as rnd\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import flax.linen as nn\n",
    "import functools\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from flax.training import train_state, checkpoints\n",
    "from flax.metrics import tensorboard\n",
    "from flax import serialization\n",
    "from random import shuffle, seed\n",
    "from typing import Sequence, Callable, Any, Optional, Dict\n",
    "from jax.lib import xla_bridge\n",
    "print('Device used', xla_bridge.get_backend().platform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcb43d8",
   "metadata": {
    "id": "bfcb43d8"
   },
   "source": [
    "## Defining the Neural Network\n",
    "\n",
    "#### python packages used to build the neural network\n",
    "Here we define the neural network. We chose to use the [Flax library](https://flax.readthedocs.io/en/latest/) built on top of JAX. [JAX](https://jax.readthedocs.io/en/latest/) is an automatic differentiation and linear algebra acceleration library developed by Google. The API is intended to mimic numpy, though it has the added capability for functional transformations like computing gradients, automatically vectorizing code, or just-in-time (JIT) compiling it down to XLA code.\n",
    "\n",
    "Flax tends to have quite a bit of complexity hidden behind the scenes which makes it a bit harder to use than comparable libraries like PyTorch. Looking through the code below, it might seem like some things are being instantiated every time the model is called, but in practice, these objects are traced when they are instantiated and the actual object being behaves slightly differently. \n",
    "\n",
    "#### the model\n",
    "The model itself consists of two sets of encoders/decoders with the linear operator acting as a coupling mechanism in the encoding space. We start off by defining the Encoder and Decoder templates:\n",
    "\n",
    "__We didn't directly parameterize the encoder/decoders in DeepGreen for this demo, so to adjust the layers/depth/activations you'll have to do it directly (or adjust the code to pass parameters to the inner modules. If you adjust the layer count, you also need to adjust the downsampling factor in GreenNet to use custom resolutions (values of n).__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181930d7",
   "metadata": {},
   "source": [
    "### encoder\n",
    "`Conv2DEncoder` builds a convolutional layer-based encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06c81cc",
   "metadata": {
    "id": "d06c81cc"
   },
   "outputs": [],
   "source": [
    "class Conv2DEncoder(nn.Module):\n",
    "    # This is a DataClass, so these implicitly define init parameters\n",
    "    num_filters: Sequence[int] = (8, 16, 32, 64)\n",
    "    conv_window: Sequence[int] = (4, 4)\n",
    "    conv_strides: int = (1,1)\n",
    "    conv_padding: str = 'SAME'\n",
    "    pool_window: Sequence[int] = (2, 2)\n",
    "    pool_strides: int = (2,2)\n",
    "    pool_padding: str = 'VALID'\n",
    "    act_fn: Callable = nn.relu\n",
    "    add_init_fin: bool = True\n",
    "        \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        inputs = x[:] # Save copy of inputs for residual        \n",
    "        for i, filters in enumerate(self.num_filters):\n",
    "            if i > 0:\n",
    "                x = nn.avg_pool(x, window_shape=self.pool_window,\n",
    "                                strides=self.pool_strides,\n",
    "                               padding=self.pool_padding)\n",
    "            x = nn.Conv(features=filters, kernel_size=self.conv_window,\n",
    "                       strides=self.conv_strides, padding=self.conv_padding)(x)\n",
    "            x = self.act_fn(x)\n",
    "            # If residual included, add projected inputs\n",
    "        if self.add_init_fin:\n",
    "            inputs = nn.Conv(features=self.num_filters[-1], \n",
    "                        kernel_size=(2**(len(self.num_filters)-1),)*2,\n",
    "                         strides=(2**(len(self.num_filters)-1),)*2,\n",
    "                        padding='VALID')(inputs)\n",
    "\n",
    "            x += inputs\n",
    "        return x.reshape(x.shape[0], -1)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37be31e",
   "metadata": {},
   "source": [
    "### decoder\n",
    "`Conv2DDecoder` builds a convolutional layer-based decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31a1316",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2DDecoder(nn.Module):\n",
    "    init_size: Sequence[int] = (16, 16, 64)\n",
    "    output_size: Sequence[int] = (-1, 128, 128)\n",
    "    num_filters: Sequence[int] = (32, 16, 8)\n",
    "    conv_window: Sequence[int] = (4, 4)\n",
    "    conv_strides:int = (2,2)\n",
    "    conv_padding: str = 'SAME'\n",
    "    act_fn: Callable = nn.relu\n",
    "    add_init_fin: bool = True\n",
    "        \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = x.reshape(-1, *self.init_size)\n",
    "        inputs = x[:]\n",
    "        for filters in self.num_filters:\n",
    "            x = nn.ConvTranspose(features=filters, kernel_size=self.conv_window,\n",
    "                                 strides=self.conv_strides, padding=self.conv_padding)(x)\n",
    "            x = self.act_fn(x)\n",
    "        # Do one last small convolution\n",
    "        x = nn.ConvTranspose(features=1,\n",
    "                             kernel_size=self.conv_window,\n",
    "                            strides=(1,1),\n",
    "                            padding=self.conv_padding)(x).squeeze(-1)\n",
    "        \n",
    "        # Note this a departure from the actual paper code\n",
    "        # which just reshapes the encoder space to add. Since\n",
    "        # that makes no sense (it's like trying to add a smaller color\n",
    "        # image to a larger greyscale image by shifting the RGB values to different\n",
    "        # pixels to make the shapes line up), we've swapped to standard upsampling\n",
    "        if self.add_init_fin:\n",
    "            inputs = nn.Conv(features=1, kernel_size=(1, 1))(inputs).squeeze(-1)\n",
    "            inputs = jax.image.resize(inputs, shape=(inputs.shape[0], *self.output_size[1:]), method='linear')\n",
    "            x += inputs\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634377d0",
   "metadata": {
    "id": "634377d0"
   },
   "source": [
    "Once the encoders and decoders are built, the full model just connects the encoder/decoder pairs through a few linear projections and the linear operator. The sequential linear operations are a little unconventional as these are essentially a single low-rank linear operator though with the coupling $Lv=f$/$L^{-1}f=v$ promoted by the loss function.\n",
    "\n",
    "Note that we return all values included in the loss functions. In applications, one would likely only be using the trained model to infer $u$ from $f$ and as such would not require all outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136b9a8c",
   "metadata": {},
   "source": [
    "### Constructing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c09302",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreenNet(nn.Module):\n",
    "    # Note, since we're only doing the nonlinear Poisson example,\n",
    "    # we removed some of the parameterization here to simplify things\n",
    "    n: int = 128\n",
    "    units_latent: int = 200    \n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, inputs):\n",
    "        u, F = inputs\n",
    "        \n",
    "        # Let's define our modules. Since we reuse them, it probably would have\n",
    "        # made more sense to not use nn.compact and just use a setup function\n",
    "        # but it is too late now. This is getting traced anyway,\n",
    "        # so it doesn't actually matter apart from looking a bit clunky.\n",
    "        u_encoder = Conv2DEncoder()\n",
    "        u_decoder = Conv2DDecoder(output_size=(-1, self.n, self.n), init_size=(self.n//8, self.n//8, 64))\n",
    "        v_reducer = nn.Dense(self.units_latent)\n",
    "        v_expander = nn.Dense(self.n**2)\n",
    "        \n",
    "        F_encoder = Conv2DEncoder()\n",
    "        F_decoder = Conv2DDecoder(output_size=(-1, self.n, self.n), init_size=(self.n//8, self.n//8, 64))\n",
    "        f_reducer = nn.Dense(self.units_latent)\n",
    "        f_expander = nn.Dense(self.n**2)\n",
    "        \n",
    "        operator = self.param('Operator', init_fn=lambda key: jnp.eye(self.units_latent)*1.)\n",
    "\n",
    "        # Autoencode u\n",
    "        u_encoded = u_encoder(u)\n",
    "        # I guess this avoids doing a large dense matrix inverse\n",
    "        v = v_reducer(u_encoded)\n",
    "        v_exp = v_expander(v)\n",
    "        u_decoded = u_decoder(v_exp)\n",
    "        \n",
    "        # Autoencode F\n",
    "        F_encoded = F_encoder(F)\n",
    "        f = f_reducer(F_encoded)\n",
    "        f_exp = f_expander(f)\n",
    "        F_decoded = F_decoder(f_exp)  \n",
    "        \n",
    "        # L things - seems kind of wasteful, but L is small\n",
    "        L_upper = jnp.triu(operator)\n",
    "        L = .5*(L_upper+L_upper.T)\n",
    "        \n",
    "        # Forward model\n",
    "        Lv = jax.lax.dot_general(v, L,\n",
    "                                 (((v.ndim - 1,), (0,)), ((), ())),)\n",
    "        Lv_exp = f_expander(Lv)\n",
    "        Lv_decoded = F_decoder(Lv_exp)\n",
    "        \n",
    "        # Inverse model\n",
    "        Linvf = jnp.linalg.solve(jnp.expand_dims(L, 0), f)\n",
    "        Linvf_exp = f_expander(Linvf)\n",
    "        Linvf_decoded = F_decoder(Linvf_exp)\n",
    "        \n",
    "        return u_decoded, F_decoded, Lv_decoded, Linvf_decoded, f, v, Lv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dff1b0",
   "metadata": {
    "id": "55dff1b0"
   },
   "source": [
    "## Building the data set\n",
    "\n",
    "We handle data a bit differently from the paper to avoid forcing everyone to download several GB of data. The paper generated data by sampling forcing functions and using an external solver to generate solutions. This is time consuming and doesn't actually compute the exact loss. We instead use the [Method of Manufactured Solutions](https://www.personal.psu.edu/jhm/ME540/lectures/VandV/MMS_summary.pdf) (MMS) so that we can generate solutions and forcings in real time.\n",
    "\n",
    "As opposed to computing true solutions for given forcings, in MMS, you sample solutions and compute the forcing, $f$, by applying the differential operator, $L$, to the solution, $u$. While this doesn't always result in realistic problems and generally requires smooth solutions, it is much, much faster and more accurate than generating data from numerical solvers. The downside is that using their forcings as solutions breaks boundary conditions which isn't ideal given the proposal is to learn a Green's function in the encoder space, so we changed some of the functions up to ensure homogeneous BCs. \n",
    "\n",
    "This data API here is a little weird - the original version was too slow for demo purposes, so the final version is a mix between the original version (which used a more standard data API) and a JIT-friendly data API. The test/train split components are the main victims here as they ultimately got hard-coded with some unnecessary storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd143ed1",
   "metadata": {
    "id": "bd143ed1"
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def nlp_operator(u):\n",
    "    \"\"\" Transforms function by applying the nonlinear Poisson operator:\n",
    "      \n",
    "        Pu = -\\nabla \\dot ((1+u^2)\\nabla u)\n",
    "    \"\"\"\n",
    "    flux = lambda x: (1+u(x)**2)*(jax.grad(u)(x))\n",
    "    return lambda x: (-jax.jacfwd(flux)(x)*jnp.eye(2)).sum()\n",
    "    \n",
    "def jax_collate(batch):\n",
    "    if isinstance(batch[0], jnp.DeviceArray):\n",
    "        return jnp.stack(batch)\n",
    "    elif isinstance(batch[0], (tuple,list)):\n",
    "        transposed = zip(*batch)\n",
    "        return [jax_collate(samples) for samples in transposed]\n",
    "    else:\n",
    "        return jnp.array(batch)\n",
    "    \n",
    "def NLP_Dataset_Sampler(n=128, batch_size=64, cubic=False, train=True, valid=False, test=False):\n",
    "    \"\"\" Factory for building data samplers for training.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    n ~ int: Number of grid points in the x and y directions\n",
    "    batch size ~ int: Examples per batch.\n",
    "    cubic ~ bool: Indicator whether to sample from the cubic space\n",
    "    train ~ bool: Indicator whether to use the train indices\n",
    "    valid ~ bool: Indicator whether to use the validation indices\n",
    "    test ~ bool: Indicator whether to use the (non-cubic) test indices\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    sampler ~ Callable: takes in indices and returns data\n",
    "    len_func ~ Callable: zero parameter function that returns the dataset size.\n",
    "    \"\"\"\n",
    "    ##### u, F Builder Functions ######\n",
    "    def gamma(k):\n",
    "        return .01 +.28*k/3\n",
    "              \n",
    "    def build_gaussian(params):\n",
    "        # Just multiplied by sin to get BCs\n",
    "        a, bx, by, c = params\n",
    "        u = lambda x: .01*a*jnp.exp((-(x[0]-bx)**2 - (x[1]-by)**2)/(2*c**2))*jnp.sin(.5*x[0])*jnp.sin(.5*x[1])\n",
    "        F = nlp_operator(u)\n",
    "        Um, Fm = jax.vmap(u), jax.vmap(F)\n",
    "        return Um(coords), Fm(coords)\n",
    "    \n",
    "    def build_cosine(params):\n",
    "        # Actually sines\n",
    "        alpha, betax, betay, _ = params \n",
    "        u = lambda x: .01*alpha*jnp.sin(betax*x[0])*jnp.sin(betay*x[1])\n",
    "        F = nlp_operator(u)\n",
    "        Um, Fm = jax.vmap(u), jax.vmap(F)\n",
    "        return Um(coords), Fm(coords)\n",
    "    \n",
    "    \n",
    "    def build_cubic1(params):\n",
    "        # Moved 0s\n",
    "        kx, ky, _, _ = params\n",
    "        u = lambda x: .01*(kx*(x[0]-jnp.pi)*(x[0]-2*jnp.pi)*x[0] \\\n",
    "                        + ky*(x[1]-jnp.pi)*(x[1]-2*jnp.pi)*x[1])\n",
    "        F = nlp_operator(u)\n",
    "        Um, Fm = jax.vmap(u), jax.vmap(F)\n",
    "        return Um(coords), Fm(coords)\n",
    "    \n",
    "    def build_cubic2(params):\n",
    "        # Moved 0s and removed psi\n",
    "        kx, ky, zetax, zetay = params\n",
    "        u = lambda x: .01*(kx*(x[0]-jnp.pi)*(x[0]-2*jnp.pi)*x[0] + ky*(x[1]-jnp.pi)*(x[1]-2*jnp.pi)*x[1] \\\n",
    "                        + zetax*(x[0]-2*jnp.pi)*x[0] + zetay*(x[1]-2*jnp.pi)*x[1])\n",
    "        F = nlp_operator(u)\n",
    "        Um, Fm = jax.vmap(u), jax.vmap(F)\n",
    "        return Um(coords), Fm(coords)\n",
    "\n",
    "    ##### Actual Sampling functions #####\n",
    "    def get_len():\n",
    "        return len(data)\n",
    "    \n",
    "    def get_item(idx):\n",
    "        sub = data[idx]\n",
    "        func_build, params = sub[0], sub[1:]\n",
    "        u, F = jax.lax.switch(func_build.astype(int), [build_gaussian, build_cosine,\n",
    "                                                      build_cubic1, build_cubic2], params) \n",
    "        return u.reshape(n, n, 1), F.reshape(n, n, 1)\n",
    "    \n",
    "    sample = jax.vmap(get_item)\n",
    "    \n",
    "    # Build shared state for all inner functions\n",
    "    if not cubic:\n",
    "        # Set up Gaussians\n",
    "        a_range = jnp.arange(-25, 26., 5)\n",
    "        b_range = jnp.arange(jnp.pi/3, 5*jnp.pi/3+.01, jnp.pi/3)\n",
    "        c_range = jnp.arange(.1, 5, .2)\n",
    "        gauss_params = [(0.,)+ p for p in product(a_range, b_range, b_range, c_range)]\n",
    "\n",
    "        alpha_range = jnp.arange(1, 10.1, .1)\n",
    "        beta_range = jnp.arange(1, 5.1, .5)\n",
    "        cosine_params = [(1.,)+ p+(0,) \n",
    "                         for p in product(alpha_range, beta_range, beta_range)]\n",
    "\n",
    "        total_params = gauss_params + cosine_params\n",
    "        seed(1)\n",
    "        shuffle(total_params)\n",
    "\n",
    "        # Hard coding the values from the paper even though it's not the same\n",
    "        if train:\n",
    "            data = total_params[:9806]\n",
    "        elif valid:\n",
    "            data = total_params[9806:9806+2452]\n",
    "        else:\n",
    "            data = total_params[9806+2452:]\n",
    "\n",
    "    else:\n",
    "        k = gamma(jnp.array([0, 1, 2, 3]))\n",
    "        zeta = jnp.arange(.01, .26, .6)\n",
    "        cubic1_params = [(2.,) + p+(0,0)\n",
    "                       for p in product(k, k)]\n",
    "        cubic2_params = [(3.,) + p\n",
    "                       for p in product(k, k, zeta, zeta)]\n",
    "        data = cubic1_params + cubic2_params\n",
    "\n",
    "    data = jnp.array(data)\n",
    "    # Build the mesh once and reuse\n",
    "    xx, yy = jnp.meshgrid(jnp.linspace(0, 2*jnp.pi, n), jnp.linspace(0, 2*jnp.pi, n))\n",
    "    coords = jnp.stack([xx, yy], -1).reshape(-1, 2)\n",
    "    \n",
    "    return jax.jit(sample), get_len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c717765",
   "metadata": {
    "id": "2c717765"
   },
   "source": [
    "## Setting up the run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a538548",
   "metadata": {
    "id": "7a538548"
   },
   "source": [
    "Now we'll define the training loops. The paper uses multiple parallel runs and takes the best performing models. This is going to be running on Colab so you are not going to have the hardware for that. Instead, we're just going to train a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f560cf10",
   "metadata": {
    "id": "f560cf10"
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update_params(state, grads):\n",
    "    return state.apply_gradients(grads=grads)\n",
    "\n",
    "        \n",
    "@jax.jit\n",
    "def compute_losses(u, F, u_decoded, F_decoded, Lv_decoded, Linvf_decoded, f, v, Lv, eps=1e-5):\n",
    "    \"\"\" Computes the 6 Normalized MSEs from the paper.\n",
    "    \n",
    "    Note: the use of means instead of sums in computing squared norms is from the paper\n",
    "    code. They also sum over batches. This doesn't matter that much, but it does adjust the weighting a bit, \n",
    "    so we manually re-weight to account for it.\n",
    "    \"\"\"\n",
    "    # Loss 1 - Reconstruction u\n",
    "    loss1 = (((u_decoded - u)**2).mean((1, 2)) / ((u**2).mean((1,2))+eps)).mean()\n",
    "    # Loss 2 - Reconstruction f \n",
    "    loss2 = (((F_decoded - F)**2).mean((1, 2)) / ((F**2).mean((1,2))+eps)).mean()\n",
    "    # Loss 3 - Forward in encoded space - don't love the gradient dynamics of dividing here\n",
    "    loss3 = (((Lv - f)**2).mean(-1) / ((f**2).mean(-1)+eps)).mean()\n",
    "    # Loss 4 - \"Superposition\" loss - seems like it would just change the relative weight of loss 3 \n",
    "    # in a weird non-convex way and penalize the magnitude of the encoded vectors. Doesn't seem like \n",
    "    # a great idea, but it is in the paper\n",
    "    loss4 = 0.\n",
    "    # Loss 5 - Forward operator loss\n",
    "    loss5 = (((Lv_decoded - F)**2).mean((1, 2)) / ((F**2).mean((1,2))+eps)).mean()\n",
    "    # Loss 6 - Backwards operator loss\n",
    "    loss6 = (((Linvf_decoded - u)**2).mean((1, 2)) / ((u**2).mean((1,2))+eps)).mean()\n",
    "    return loss1, loss2, loss3, loss4, loss5, loss6\n",
    "\n",
    "@functools.partial(jax.jit, static_argnums=(0, 2, 4, 5))\n",
    "def train_epoch(model, state, data, perms, num_batches, eval_full=True):\n",
    "    # JIT is touchy around conditionals (the trace notes the first path), so this just says compile\n",
    "    #  again if the conditional changes. \n",
    "    @functools.partial(jax.jit)\n",
    "    def grad_step(state, perm):\n",
    "        def loss_fn(params):\n",
    "            # Run model\n",
    "            u_decoded, F_decoded, Lv_decoded, Linvf_decoded, f, v, Lv = model.apply({'params':params}, batch)\n",
    "            u, F = batch\n",
    "            u, F = u.squeeze(-1), F.squeeze(-1)\n",
    "            loss1, loss2, loss3, loss4, loss5, loss6 = compute_losses(u, F, u_decoded, F_decoded, \n",
    "                                                                      Lv_decoded, Linvf_decoded, f, v, Lv)\n",
    "            # We're keeping all the losses for logging, but the total loss is only the reconstruction\n",
    "            # losses at the beginning of training\n",
    "            if eval_full:\n",
    "                return loss1+loss2+loss3+loss4+loss5+loss6, (loss1, loss2, loss3, loss4, loss5, loss6)\n",
    "            else:\n",
    "                return loss1+loss2, (loss1, loss2, loss3, loss4, loss5, loss6)\n",
    "        \n",
    "        batch = data(perm)\n",
    "        grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "        (train_loss, aux_losses), grads = grad_fn(state.params)\n",
    "        state = update_params(state, grads)\n",
    "        return state, aux_losses\n",
    "\n",
    "    # Run actual code - Carries and updates state every batch\n",
    "    # returns final state and accumulated outputs (losses)\n",
    "    state, aux_loss_means = jax.lax.scan(grad_step, state, perms)\n",
    "    return state, [jnp.mean(l) for l in aux_loss_means]   \n",
    "\n",
    "def validation_run(model, state, data, batch_size, num_batches):\n",
    "    # Only difference is we do not compute the gradient and update the state here\n",
    "    @functools.partial(jax.jit)\n",
    "    def exec_step(state, perm):\n",
    "        # Run model\n",
    "        batch = data(perm)\n",
    "        u_decoded, F_decoded, Lv_decoded, Linvf_decoded, f, v, Lv = model.apply({'params':state.params}, batch)\n",
    "        u, F = batch\n",
    "        u, F = u.squeeze(-1), F.squeeze(-1)\n",
    "        loss1, loss2, loss3, loss4, loss5, loss6 = compute_losses(u, F, u_decoded, F_decoded, \n",
    "                                                                  Lv_decoded, Linvf_decoded, f, v, Lv)        \n",
    "        return state, (loss1, loss2, loss3, loss4, loss5, loss6)\n",
    "    perms = jnp.arange(batch_size*num_batches).reshape(num_batches, batch_size)\n",
    "    state, aux_loss_means = jax.lax.scan(exec_step, state, perms)\n",
    "    return [jnp.mean(l) for l in aux_loss_means] \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7592d8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model run parameters\n",
    "batch_size = 64\n",
    "n= 128\n",
    "\n",
    "# Content doesn't matter so we're just chosing a convenient shape\n",
    "init_vals = jnp.array(np.random.uniform(size=(10, n, n, 1)))\n",
    "\n",
    "# JAX's random is kind of a pain, but it makes more sense when you're doing distributed training\n",
    "# which we are not doing here. \n",
    "key = rnd.PRNGKey(0)\n",
    "key, use_key = rnd.split(key, 2)\n",
    "\n",
    "model = GreenNet(n=n)\n",
    "params = model.init(use_key, (init_vals, init_vals))\n",
    "\n",
    "# JAX is a functional framework, but Deep Learning has historically used mostly object oriented libraries.\n",
    "# TrainState is a helper for organizing the model parameters and state so the training loop can be written \n",
    "# more like OOP code.\n",
    "\n",
    "# We're using the AdamW optimizer as the paper uses Adam with weight decay. \n",
    "state = train_state.TrainState.create(apply_fn=model.apply,\n",
    "                                     params = params['params'],\n",
    "                                     tx=optax.adamw(learning_rate=1e-3, weight_decay=1e-6))\n",
    "\n",
    "# Load up on datasets - the len funcs instead of objects were just a mistake\n",
    "train_dataset, train_len_func = NLP_Dataset_Sampler(n=n, batch_size=batch_size)\n",
    "valid_dataset, valid_len_func = NLP_Dataset_Sampler(n=n, batch_size=batch_size, train=False, test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76d99c6",
   "metadata": {},
   "source": [
    "Before we train the model, let's look in at what functions we're trying to learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e413d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "u, F = train_dataset(jnp.array([0, 14]))\n",
    "u, F = u.squeeze(-1), F.squeeze(-1)\n",
    "\n",
    "fig = plt.figure(constrained_layout=True, figsize=(6,6))\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 6))\n",
    "axes[0][0].set_title('u Values')\n",
    "axes[0][1].set_title('F Values')\n",
    "aa = axes[0][0].imshow(u[0])\n",
    "plt.colorbar(aa, ax=axes[0][0])\n",
    "ab = axes[0][1].imshow(F[0])\n",
    "plt.colorbar(ab, ax=axes[0][1])\n",
    "ac = axes[1][0].imshow(u[1])\n",
    "plt.colorbar(ac, ax=axes[1][0])\n",
    "ad = axes[1][1].imshow(F[1])\n",
    "plt.colorbar(ad, ax=axes[1][1])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3141216a",
   "metadata": {},
   "source": [
    "These end up being fairly similar to the problems from the paper, though the scaling is a bit off as some $u$ and $F$ values are separated by several orders of magnitude while even the worst performing cases in the paper were generally 1 or fewer orders of magnitude apart. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c22967a",
   "metadata": {},
   "source": [
    "## Creating the Model\n",
    "\n",
    "Now all that's left is training the model. On the colab GPU with $n=128$ (the setting from the paper), this will take far longer than the length of our class period. To run the model, try different validation batch sizes by adjusting `valid_bs` (make sure it's a multiple of 16).\n",
    "\n",
    "While you won't be able to train an entire model during the class time activity, go ahead and run the tensorboard cell to visualize the model training. Note that the paper only uses the autoencoder losses for the first 75 epochs of training, so you'll notice a huge drop in loss after that point as L6 seems to be the largest loss in most of our runs. \n",
    "\n",
    "You won't get to 75 epochs during this class demo, after starting/visualizing the training, you can stop it and move on  to loading in the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17671bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7d96ab",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "3f7d96ab",
    "outputId": "6ba0e227-d59d-4c74-fbdf-60cfef8b5f72",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Train housekeeping\n",
    "train_size = train_len_func()\n",
    "n_batches = train_size // batch_size\n",
    "\n",
    "#Validation housekeeping\n",
    "valid_size = valid_len_func()\n",
    "valid_bs = 512\n",
    "valid_nbatches = valid_size // valid_bs\n",
    "\n",
    "# General run things\n",
    "aec_only_epochs = 75\n",
    "full_epochs = 2500\n",
    "train_writer = tensorboard.SummaryWriter('train')\n",
    "valid_writer = tensorboard.SummaryWriter('valid')\n",
    "log_rate = 10\n",
    "loss_names= ['L'+str(i) for i in range(1, 7)]\n",
    "key = rnd.PRNGKey(0)\n",
    "\n",
    "for i in range(full_epochs):\n",
    "    # Run the training epoch\n",
    "    key, use_key = rnd.split(key, 2)\n",
    "    perm = rnd.permutation(use_key, train_size)[:batch_size*n_batches].reshape(n_batches, batch_size)\n",
    "    state, train_loss = train_epoch(model, state, train_dataset, perm, n_batches, eval_full=i>aec_only_epochs)\n",
    "    # Check the validation set\n",
    "    valid_losses = validation_run(model, state, valid_dataset, valid_bs, valid_nbatches)\n",
    "    # Logging\n",
    "    for j, loss in enumerate(train_loss):\n",
    "        train_writer.scalar(loss_names[j], loss, i)\n",
    "        valid_writer.scalar(loss_names[j], valid_losses[j], i)\n",
    "    train_writer.scalar('Total loss', jnp.sum(jnp.array(train_loss)), i)\n",
    "    valid_writer.scalar('Total loss', jnp.sum(jnp.array(valid_losses)), i)  \n",
    "    if i % log_rate == 0:\n",
    "        print('Epoch', i, 'Total validation loss:', jnp.sum(jnp.array(valid_losses)))\n",
    "        checkpoints.save_checkpoint('checkpoints', state, i, keep=3, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714deaf7",
   "metadata": {},
   "source": [
    "## Experimenting with a pre-trained model\n",
    "\n",
    "Instead of waiting for your model to train, load in a pre-trained model to experiment on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e7697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just define this plot once\n",
    "def compare_plot(u, F, u_pred, F_pred):\n",
    "    u, F, u_pred, F_pred = u.squeeze(), F.squeeze(), u_pred.squeeze(), F_pred.squeeze()\n",
    "    # Axes\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 6))\n",
    "    axes[0][0].set_title('(True)')\n",
    "    axes[0][1].set_title('(Pred)')\n",
    "    axes[0][2].set_title('(Diff)')\n",
    "    axes[0][0].set_ylabel('u')\n",
    "    axes[1][0].set_ylabel('F')\n",
    "    # u's\n",
    "    aa = axes[0][0].imshow(u)\n",
    "    plt.colorbar(aa, ax=axes[0][0])\n",
    "    ab = axes[0][1].imshow(u_pred)\n",
    "    plt.colorbar(ab, ax=axes[0][1])\n",
    "    ac = axes[0][2].imshow(u - u_pred)\n",
    "    plt.colorbar(ac, ax=axes[0][2])\n",
    "    # Fs\n",
    "    ba = axes[1][0].imshow(F)\n",
    "    plt.colorbar(ba, ax=axes[1][0])\n",
    "    bb = axes[1][1].imshow(F_pred)\n",
    "    plt.colorbar(bb, ax=axes[1][1])\n",
    "    bc = axes[1][2].imshow(F-F_pred)\n",
    "    plt.colorbar(bc, ax=axes[1][2])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b198a8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment this to use pretrained model - this will overwrite the model trained above, \n",
    "# though that can also be restored from a checkpoint\n",
    "n = 128\n",
    "init_vals = jnp.array(np.random.uniform(size=(10, n, n, 1)))\n",
    "key = rnd.PRNGKey(0)\n",
    "key, use_key = rnd.split(key, 2)\n",
    "model = GreenNet(n=n)\n",
    "params = model.init(use_key, (init_vals, init_vals))\n",
    "# Path might be different\n",
    "with open('saved_model.p', 'rb') as f:    \n",
    "    params = serialization.from_bytes(params, pickle.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be5495d",
   "metadata": {},
   "source": [
    "### Explore the test data\n",
    "\n",
    "These are cases from the same family of distributions that we trained on, but with different parameters. Realistically, these should be close enough to the training data that we wouldn't expect to see large differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ae2025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate cell since it has an initial cost to generate\n",
    "test_dataset, test_len_func = NLP_Dataset_Sampler(n=n, batch_size=batch_size, train=False, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4ec019",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 15\n",
    "u, F = test_dataset(jnp.array([index]))\n",
    "_, _, F_pred, u_pred, _, _, _ = model.apply(params, (u, F))\n",
    "compare_plot(u, F, u_pred, F_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff86deb0",
   "metadata": {},
   "source": [
    "### Running on a different family of solutions (cubics)\n",
    "\n",
    "These are families of functions never seen during training so we'd expect worse performance. In our case it performs quite a bit worse, but unlike the paper's models, these models were also not trained to convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb018c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cubic_test_dataset, cubic_len_func = NLP_Dataset_Sampler(n=n, batch_size=batch_size, cubic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611b2de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 30\n",
    "u, F = cubic_test_dataset(jnp.array([index]))\n",
    "_, _, F_pred, u_pred, _, _, _ = model.apply(params, (u, F))\n",
    "\n",
    "compare_plot(u, F, u_pred, F_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fa3a0e",
   "metadata": {},
   "source": [
    "### Try your own function\n",
    "\n",
    "The code is set-up so that you can actually test arbitrary functions. Since we're getting test cases from the forward operator, define your own $u$ and see how the model performs on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58a3142",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx, yy = jnp.meshgrid(jnp.linspace(0, 2*jnp.pi, n), jnp.linspace(0, 2*jnp.pi, n))\n",
    "coords = jnp.stack([xx, yy], -1).reshape(-1, 2)\n",
    "\n",
    "# First we define an arbitrary function (should have 0 BCs, but the code will still run either way)\n",
    "ufunc = lambda x: .01*((x[0]-jnp.pi)*(x[0]-2*jnp.pi)*x[0] + (x[1]-jnp.pi)*(x[1]-2*jnp.pi)*x[1] \\\n",
    "                + (x[0]-2*jnp.pi)*x[0] + (x[1]-2*jnp.pi)*x[1])\n",
    "# We use algorithmic differentiation to apply the differential operator\n",
    "Ffunc = nlp_operator(ufunc)\n",
    "# Next we vmap our functions to vectorize them\n",
    "Um, Fm = jax.vmap(ufunc), jax.vmap(Ffunc)\n",
    "\n",
    "# Need to be the right shape to feed through the network\n",
    "u, F = Um(coords).reshape(1, 128, 128, 1), Fm(coords).reshape(1, 128, 128, 1)\n",
    "\n",
    "_, _, F_pred, u_pred, _, _, _ = model.apply(params, (u, F))\n",
    "compare_plot(u, F, u_pred, F_pred)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DeepGreen_rough.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
